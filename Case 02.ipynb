{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.vectorizer = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
    "        self.corpus = []\n",
    "        self.load_corpus()\n",
    "\n",
    "    def fit_transform(self):\n",
    "        return self.vectorizer.fit_transform(self.corpus)\n",
    "    \n",
    "    def transform(self):\n",
    "        return self.vectorizer.transform(self.corpus)\n",
    "\n",
    "    def load_corpus(self):\n",
    "        with open('corpus.txt', 'r') as file:\n",
    "           self.corpus = file.readlines()\n",
    "\n",
    "    def get_corpus(self):\n",
    "        return self.corpus\n",
    "\n",
    "def calcuate_cosine_similarity(matrix, query_v):\n",
    "    similarities = cosine_similarity(query_v, matrix)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Gram Model: \n",
      "   000  11  13  1879  1889  1969  19th  ada  against  agra  ...  wall  was  \\\n",
      "0    0   0   0     0     1     0     0    0        0     0  ...     0    1   \n",
      "1    0   1   0     0     0     1     0    0        0     0  ...     0    0   \n",
      "2    0   0   0     1     0     0     0    0        0     0  ...     0    1   \n",
      "3    0   0   0     0     0     0     0    0        0     0  ...     0    0   \n",
      "4    1   0   1     0     0     0     0    0        1     0  ...     1    1   \n",
      "5    0   0   0     0     0     0     0    0        0     0  ...     0    0   \n",
      "6    0   0   0     0     0     0     0    0        0     1  ...     0    0   \n",
      "7    0   0   0     0     0     0     1    1        0     0  ...     0    0   \n",
      "8    0   0   0     0     0     0     0    0        0     0  ...     0    0   \n",
      "9    0   0   0     0     0     0     0    0        0     0  ...     0    0   \n",
      "\n",
      "   widely  wildebeest  william  with  worked  works  world  writers  \n",
      "0       0           0        0     0       0      0      0        0  \n",
      "1       0           0        0     0       0      0      0        0  \n",
      "2       0           0        0     0       0      0      0        0  \n",
      "3       0           0        0     0       0      0      0        0  \n",
      "4       0           0        0     0       0      0      1        0  \n",
      "5       1           0        1     0       0      1      0        1  \n",
      "6       0           0        0     0       0      0      1        0  \n",
      "7       0           0        0     0       1      0      1        0  \n",
      "8       0           1        0     0       0      0      0        0  \n",
      "9       1           0        0     1       0      0      0        0  \n",
      "\n",
      "[10 rows x 143 columns]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "ngram_model = NGramLanguageModel(n)\n",
    "\n",
    "\n",
    "matrix = ngram_model.fit_transform()\n",
    "query_v = ngram_model.transform()\n",
    "\n",
    "print(f'{n}-Gram Model: ')\n",
    "data = matrix.A\n",
    "\n",
    "print(pd.DataFrame(matrix.A, columns=ngram_model.vectorizer.get_feature_names_out()))\n",
    "print(query_v.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Eiffel Tower is located in Paris, France. ...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 1969, Neil Armstrong and Buzz Aldrin became...</td>\n",
       "      <td>0.247436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albert Einstein, a theoretical physicist, deve...</td>\n",
       "      <td>0.308607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Amazon Rainforest is home to a diverse ran...</td>\n",
       "      <td>0.346467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Great Wall of China is a world-renowned fo...</td>\n",
       "      <td>0.279145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>William Shakespeare, an English playwright, is...</td>\n",
       "      <td>0.299392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Taj Mahal, located in Agra, India, is a UN...</td>\n",
       "      <td>0.370479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ada Lovelace is considered the world's first c...</td>\n",
       "      <td>0.279145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Serengeti National Park in Tanzania is fam...</td>\n",
       "      <td>0.370479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Michael Jordan, a basketball legend, played th...</td>\n",
       "      <td>0.168359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document  Similarity\n",
       "0  The Eiffel Tower is located in Paris, France. ...    1.000000\n",
       "1  In 1969, Neil Armstrong and Buzz Aldrin became...    0.247436\n",
       "2  Albert Einstein, a theoretical physicist, deve...    0.308607\n",
       "3  The Amazon Rainforest is home to a diverse ran...    0.346467\n",
       "4  The Great Wall of China is a world-renowned fo...    0.279145\n",
       "5  William Shakespeare, an English playwright, is...    0.299392\n",
       "6  The Taj Mahal, located in Agra, India, is a UN...    0.370479\n",
       "7  Ada Lovelace is considered the world's first c...    0.279145\n",
       "8  The Serengeti National Park in Tanzania is fam...    0.370479\n",
       "9  Michael Jordan, a basketball legend, played th...    0.168359"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = calcuate_cosine_similarity(matrix, query_v)\n",
    "\n",
    "data = {'Document': ngram_model.get_corpus(), 'Similarity': similarities[0]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Eiffel Tower is located in Paris, France. ...</td>\n",
       "      <td>0.473185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 1969, Neil Armstrong and Buzz Aldrin became...</td>\n",
       "      <td>0.406962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albert Einstein, a theoretical physicist, deve...</td>\n",
       "      <td>0.595622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Amazon Rainforest is home to a diverse ran...</td>\n",
       "      <td>0.299586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Great Wall of China is a world-renowned fo...</td>\n",
       "      <td>0.230761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>William Shakespeare, an English playwright, is...</td>\n",
       "      <td>0.377185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Taj Mahal, located in Agra, India, is a UN...</td>\n",
       "      <td>0.620114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ada Lovelace is considered the world's first c...</td>\n",
       "      <td>0.118555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Serengeti National Park in Tanzania is fam...</td>\n",
       "      <td>0.123411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Michael Jordan, a basketball legend, played th...</td>\n",
       "      <td>0.264627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document  Similarity\n",
       "0  The Eiffel Tower is located in Paris, France. ...    0.473185\n",
       "1  In 1969, Neil Armstrong and Buzz Aldrin became...    0.406962\n",
       "2  Albert Einstein, a theoretical physicist, deve...    0.595622\n",
       "3  The Amazon Rainforest is home to a diverse ran...    0.299586\n",
       "4  The Great Wall of China is a world-renowned fo...    0.230761\n",
       "5  William Shakespeare, an English playwright, is...    0.377185\n",
       "6  The Taj Mahal, located in Agra, India, is a UN...    0.620114\n",
       "7  Ada Lovelace is considered the world's first c...    0.118555\n",
       "8  The Serengeti National Park in Tanzania is fam...    0.123411\n",
       "9  Michael Jordan, a basketball legend, played th...    0.264627"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus = []\n",
    "with open('corpus.txt', 'r') as file:\n",
    "    corpus = file.readlines()\n",
    "\n",
    "query = 'I was born in 2005 in California, USA, I go to binus university and I am a part-time lab teaching assistant'\n",
    "\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "tokenized_query = word_tokenize(query.lower())\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "similarities = []\n",
    "\n",
    "for token in tokenized_corpus:\n",
    "    similarity = model.wv.n_similarity(tokenized_query, token)\n",
    "    similarities.append(similarity)\n",
    "\n",
    "df = pd.DataFrame({'Document': corpus, 'Similarity': similarities})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: i have a gun\n",
      "(S (NP i) (VP (V have) (NP (Det a) (N gun)))) \n",
      "\n",
      "          S             \n",
      "  ________|___           \n",
      " |            VP        \n",
      " |    ________|___       \n",
      " |   |            NP    \n",
      " |   |         ___|___   \n",
      " NP  V       Det      N \n",
      " |   |        |       |  \n",
      " i  have      a      gun\n",
      "\n",
      "\n",
      "Sentence 2: the building is huge\n",
      "No Parses Found\n",
      "\n",
      "Sentence 3: i lost my wallet\n",
      "(S (NP i) (VP (V lost) (NP (Det my) (N wallet)))) \n",
      "\n",
      "          S                \n",
      "  ________|___              \n",
      " |            VP           \n",
      " |    ________|___          \n",
      " |   |            NP       \n",
      " |   |         ___|____     \n",
      " NP  V       Det       N   \n",
      " |   |        |        |    \n",
      " i  lost      my     wallet\n",
      "\n",
      "\n",
      "Sentence 4: the person ran away with my wallet\n",
      "No Parses Found\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import CFG\n",
    "from nltk.parse import ChartParser\n",
    "\n",
    "# CFG = set of recursive rules used to generate patterns of strings\n",
    "def demonstrate_nlp_parsing(sentence, grammar):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    parser = ChartParser(grammar)\n",
    "\n",
    "    parses = list(parser.parse(words))\n",
    "    if parses:\n",
    "        for tree in parser.parse(sentence.split()):\n",
    "            print(tree, '\\n')\n",
    "            tree.pretty_print()\n",
    "    else:\n",
    "        print(\"No Parses Found\")\n",
    "\n",
    "nlp_grammar = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N | Det N PP | 'i'\n",
    "    VP -> V NP | V NP PP | V\n",
    "    Det -> 'the' | 'a' | 'my'\n",
    "    N -> 'person' | 'wallet' | 'building' | 'gun' | 'fire'\n",
    "    V -> 'saw' | 'ate' | 'chased' | 'ran' | 'lost' | 'went' | 'have' | 'VBP' | 'caught' | 'is'\n",
    "    PP -> P NP\n",
    "    P -> 'in' | 'on' | 'with' | 'from'\n",
    "    Adj -> 'big' | 'away' | 'huge'\n",
    "\"\"\")\n",
    "\n",
    "nlp_sentences = [\n",
    "    'i have a gun',\n",
    "    'the building is huge',\n",
    "    'i lost my wallet',\n",
    "    'the person ran away with my wallet'\n",
    "]\n",
    "\n",
    "for i, sentence in enumerate(nlp_sentences):\n",
    "    print(f'Sentence {i+1}: {sentence}')\n",
    "    demonstrate_nlp_parsing(sentence, nlp_grammar)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Dependency Parse Tree:\n",
      "I --nsubjpass--> born (PRON)\n",
      "was --auxpass--> born (AUX)\n",
      "born --ccomp--> go (VERB)\n",
      "in --prep--> born (ADP)\n",
      "2005 --pobj--> in (NUM)\n",
      "in --prep--> born (ADP)\n",
      "California --pobj--> in (PROPN)\n",
      ", --punct--> California (PUNCT)\n",
      "USA --appos--> California (PROPN)\n",
      ", --punct--> go (PUNCT)\n",
      "I --nsubj--> go (PRON)\n",
      "go --ROOT--> go (VERB)\n",
      "to --aux--> binus (PART)\n",
      "binus --advcl--> go (VERB)\n",
      "university --dobj--> binus (PROPN)\n",
      "and --cc--> go (CCONJ)\n",
      "I --nsubj--> am (PRON)\n",
      "am --conj--> go (AUX)\n",
      "a --det--> lab (DET)\n",
      "part --amod--> time (ADJ)\n",
      "- --punct--> time (PUNCT)\n",
      "time --compound--> lab (NOUN)\n",
      "lab --attr--> am (NOUN)\n",
      "teaching --acl--> lab (VERB)\n",
      "assistant --dobj--> teaching (NOUN)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def extract_named_entities_and_parse_tree(sentence):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    print(\"Formatted Dependency Parse Tree:\")\n",
    "    for token in doc:\n",
    "        print(f'{token.text} --{token.dep_}--> {token.head.text} ({token.pos_})')\n",
    "\n",
    "extract_named_entities_and_parse_tree('I was born in 2005 in California, USA, I go to binus university and I am a part-time lab teaching assistant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAC: ['The Eiffel Tower', 'the moon during', 'The Great Wall of China']\n",
      "GPE: ['Paris', 'France', 'Germany', 'Juliet', 'Agra', 'India', 'Tanzania']\n",
      "DATE: ['1889', '1969', '1879', 'the 19th century', 'annual']\n",
      "PERSON: ['Neil Armstrong', 'Buzz Aldrin', 'Albert Einstein', 'William Shakespeare', 'Romeo', 'Ada Lovelace', 'Analytical Engine', 'Michael Jordan']\n",
      "ORDINAL: ['first']\n",
      "LAW: ['Apollo 11']\n",
      "ORG: ['Amazon', 'UNESCO World Heritage Site', \"Charles Babbage's\", 'The Serengeti National Park', 'the Chicago Bulls']\n",
      "LOC: ['Earth']\n",
      "QUANTITY: ['over 13,000 miles']\n",
      "LANGUAGE: ['English']\n",
      "WORK_OF_ART: ['Hamlet', 'The Taj Mahal']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def extract_named_entities(sentence):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    categories = {}\n",
    "    for ent in doc.ents:\n",
    "        label = ent.label_\n",
    "        if label not in categories:\n",
    "            categories[label] = []\n",
    "        if ent.text not in categories[label]:\n",
    "            categories[label].append(ent.text)\n",
    "\n",
    "    for label, entities in categories.items():\n",
    "        print(f\"{label}: {entities}\")\n",
    "\n",
    "corpus = []\n",
    "with open('corpus.txt', 'r') as file:\n",
    "    corpus = file.readlines()\n",
    "\n",
    "sentence = ' '.join(corpus)\n",
    "sentence = ''.join([s.replace('\\n', '') for s in sentence])\n",
    "\n",
    "extract_named_entities(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
